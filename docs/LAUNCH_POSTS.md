# SafeBrowse â€” Launch Assets

Complete launch materials for SafeBrowse v0.2.0.

---

## ğŸ“¸ Screenshot & Demo Captions

Use these exact captions for screenshots / GIFs:

### Screenshot 1 â€” Malicious Page
```
This webpage looks normal â€” but contains hidden prompt injection.
```

### Screenshot 2 â€” Extension Block
```
SafeBrowse blocks the request before the AI sees it.
```

### Screenshot 3 â€” SDK Block
```
The agent never runs. Execution is stopped by SafeBrowse.
```

### Screenshot 4 â€” Audit Log
```
Every decision is logged with a request ID for audit and compliance.
```

### Screenshot 5 â€” RAG Sanitization
```
Poisoned data is removed before entering the vector database.
```

---

## ğŸ”¥ Hacker News Post

### Title
```
Show HN: A Prompt-Injection Firewall for AI Agents and RAG Pipelines
```

### Post Body
```
We built SafeBrowse â€” an open-source prompt-injection firewall for AI systems.

Instead of relying on better prompts, SafeBrowse enforces a hard security boundary between untrusted web content and LLMs.

It blocks hidden instructions, policy violations, and poisoned data before the AI ever sees it.

Features:
â€¢ Prompt injection detection (50+ patterns)
â€¢ Policy engine (login/payment blocking)
â€¢ Fail-closed by design
â€¢ Audit logs & request IDs  
â€¢ Python SDK (sync + async)
â€¢ RAG sanitization

Demo: [DEMO_LINK]
GitHub: [GITHUB_LINK]
PyPI: pip install safebrowse

Looking for feedback from AI infra, security, and agent builders.
```

---

## ğŸ”— LinkedIn Post

### Founder / Tech Lead Tone
```
AI systems are now browsing the web and ingesting data autonomously.

The web is not safe for AI.

We built SafeBrowse, a prompt-injection firewall that blocks malicious content before it reaches your LLMs, agents, or RAG pipelines.

This isn't a prompt trick.
It's an enforceable security boundary.

âœ“ Fail-closed by design
âœ“ Audit-ready
âœ“ Python SDK
âœ“ Open-source

â–¶ Watch the demo: [DEMO_LINK]
â–¶ pip install safebrowse

Would love feedback from AI infra and security engineers.

#AIAgent #LLM #Security #PromptInjection #OpenSource #Python
```

---

## ğŸ¦ Twitter/X Thread

### Tweet 1 (Hook)
```
ğŸ›¡ï¸ Introducing SafeBrowse

A prompt-injection firewall for AI agents.

The web is not safe for AI.
We built a solution.

ğŸ§µ Thread:
```

### Tweet 2 (Problem)
```
The problem:

AI agents and RAG pipelines ingest untrusted web content.

Hidden instructions can hijack LLM behavior â€” without humans ever seeing it.

Prompting alone cannot solve this.
```

### Tweet 3 (Solution)
```
The solution:

SafeBrowse enforces a hard security boundary.

Before: Web â†’ LLM â†’ Hope nothing bad happens

After: Web â†’ SafeBrowse â†’ LLM

The AI never sees malicious content.
```

### Tweet 4 (Demo)
```
See it in action:

âœ“ Scans content before your AI
âœ“ Blocks prompt injection (50+ patterns)
âœ“ Blocks login/payment forms
âœ“ Sanitizes RAG chunks

[DEMO_GIF]
```

### Tweet 5 (SDK)
```
The SDK:

from safebrowse import SafeBrowseClient

with client.guard(html, url):
    agent.run()  # Only runs if safe

No silent failures. No bypasses.
Fail-closed by design.
```

### Tweet 6 (CTA)
```
Ready to protect your AI?

ğŸ“¦ pip install safebrowse
â­ github.com/safebrowse/safebrowse-python

Open source. Enterprise ready.

Feedback welcome ğŸ™
```

---

## ğŸ“§ Product Hunt Tagline

```
SafeBrowse â€” Prompt-injection firewall for AI agents
```

### One-liner
```
Block malicious web content before it reaches your AI. Fail-closed by design.
```

### Maker Comment
```
Hey Product Hunt! ğŸ‘‹

We built SafeBrowse because AI agents are now browsing the web autonomously - but the web is full of content designed to hijack AI systems.

Instead of relying on clever prompts (which can be bypassed), SafeBrowse enforces a hard security boundary between untrusted content and your LLMs.

Key features:
â€¢ 50+ prompt injection patterns detected
â€¢ Policy engine for login/payment forms
â€¢ RAG sanitization for vector DBs
â€¢ Python SDK with sync/async support
â€¢ Fail-closed: security cannot be bypassed

We'd love your feedback! What security challenges are you facing with AI agents?
```
